# Load necessary libraries
library(caret)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Evaluate the model on the test data
logistic_predictions <- predict(logistic_model, newdata = test_data, type = "response")
# Load necessary libraries
# Load necessary libraries
library(caret)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Evaluate the model on the test data
logistic_predictions <- predict(logistic_model, newdata = test_data, type = "lm")
# Load necessary libraries
# Load necessary libraries
library(caret)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Evaluate the model on the test data
logistic_predictions <- predict(logistic_model, newdata = test_data, type = "prob")
# Load necessary libraries
library(caret)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Predict probabilities
logistic_probabilities <- predict(logistic_model, newdata = test_data, type = "response")
# Load necessary libraries
library(caret)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Predict probabilities
logistic_probabilities <- predict(logistic_model, newdata = test_data, type = "raw")
# Convert probabilities to binary predictions
logistic_predictions <- ifelse(logistic_probabilities > 0.5, 1, 0)
# Evaluate the model on the test data
logistic_performance <- confusionMatrix(logistic_predictions, test_data$target)
# Load necessary libraries
library(caret)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Predict probabilities
logistic_probabilities <- predict(logistic_model, newdata = test_data, type = "raw")
# Convert probabilities to binary predictions
logistic_predictions <- ifelse(logistic_probabilities > 0.5, 1, 0)
# Evaluate the model on the test data
logistic_performance <- confusionMatrix(logistic_predictions, test_data$target)
# Load necessary libraries
library(caret)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Predict probabilities
logistic_probabilities <- predict(logistic_model, newdata = test_data, type = "raw")
# Convert probabilities to binary predictions
logistic_predictions <- ifelse(logistic_probabilities > 0.5, 1, 0)
# Evaluate the model on the test data
logistic_performance <- confusionMatrix(logistic_predictions, test_data$Y)
# Load necessary libraries
library(caret)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Predict probabilities
logistic_probabilities <- predict(logistic_model, newdata = test_data, type = "raw")
# Convert probabilities to binary predictions
logistic_predictions <- ifelse(logistic_probabilities > 0.5, 1, 0)
print(logistic_model)
# Interpret and explain the logistic regression model
# Analyze the impact of "subscribers" on the binary target
install.packages("glm2")
# Load necessary libraries
library(caret)
library(glm2)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Predict probabilities
logistic_probabilities <- predict(logistic_model, newdata = test_data, type = "raw")
# Convert probabilities to binary predictions
logistic_predictions <- ifelse(logistic_probabilities > 0.5, 1, 0)
print(logistic_model)
# Interpret and explain the logistic regression model
# Analyze the impact of "subscribers" on the binary target
# Load necessary libraries
library(caret)
library(glm2)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial", nIter = 1000)
# Load necessary libraries
library(caret)
library(glm2)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Predict probabilities
logistic_probabilities <- predict(logistic_model, newdata = test_data, type = "raw")
# Convert probabilities to binary predictions
logistic_predictions <- ifelse(logistic_probabilities > 0.5, 1, 0)
print(logistic_model)
# Interpret and explain the logistic regression model
# Analyze the impact of "subscribers" on the binary target
# Load necessary libraries
library(caret)
library(party)  # For Decision Tree
library(e1071)  # For SVM (example of a different classifier)
# Preprocess and merge data
# Perform any necessary data cleaning and feature engineering
# Merge the two datasets based on a common identifier (e.g., country)
# Create a target variable (e.g., "high earnings" or "low earnings")
# Based on your assumptions and GDP per capita data
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$Y, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Single-Variate Model - Decision Tree Classifier
decision_tree_model <- train(Y ~ ., data = train_data, method = "ctree")
# Evaluate Decision Tree Model
decision_tree_predictions <- predict(decision_tree_model, newdata = test_data)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(ggthemes)
library(numform)
library(timeDate)
library(lubridate)
library(reshape2)
library(ca)
library(tidyr)
library(ape)
library(knitr)
library(ROCR)
library(rpart)
library(grid)
library(gridExtra)
library(tidyverse)
library(data.table)
library(xgboost)
library(vtreat)
# Load necessary libraries
library(caret)
# Create a binary target variable for classification based on 'subscribers'
data$high_subscribers <- ifelse(data$subscribers > 100000, "High", "Low")
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$high_subscribers, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Build a decision tree model for classification
decision_tree_model <- train(high_subscribers ~ subscribers, data = train_data, method = "rpart")
# Load necessary libraries
library(caret)
# Create a binary target variable for classification based on 'subscribers'
data$high_subscribers <- ifelse(data$subscribers > 100000, "High", "Low")
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$high_subscribers, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Build a decision tree model for classification
decision_tree_model <- train(high_subscribers ~ subscribers, data = train_data, method = "row")
# Load necessary libraries
library(caret)
# Create a binary target variable for classification based on 'subscribers'
data$high_subscribers <- ifelse(data$subscribers > 100000, "High", "Low")
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$high_subscribers, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Build a decision tree model for classification
decision_tree_model <- train(high_subscribers ~ subscribers, data = train_data, method = "rpart")
# Load necessary libraries
library(caret)
# Create a binary target variable for classification based on 'subscribers'
data$high_subscribers <- ifelse(data$subscribers > 100000, "High", "Low")
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$high_subscribers, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Build a decision tree model for classification
decision_tree_model <- train(high_subscribers ~ subscribers, data = train_data, method = "rpart")
# Load necessary libraries
library(caret)
# Create a binary target variable for classification based on 'subscribers'
data$high_subscribers <- ifelse(data$subscribers > 100000, "High", "Low")
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$high_subscribers, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
train_data
# Build a decision tree model for classification
decision_tree_model <- train(high_subscribers ~ subscribers, data = train_data, method = "rpart")
# Build a decision tree model for classification
decision_tree_model <- train(train_data, data = train_data, method = "rpart")
# Build a decision tree model for classification
decision_tree_model <- train(train_data$Y, data = train_data, method = "rpart")
# Build a decision tree model for classification
decision_tree_model <- train(train_data$Y~, data = train_data, method = "rpart")
# Build a decision tree model for classification
decision_tree_model <- train(train_data$Y, data = train_data, method = "rpart")
# Build a decision tree model for classification
# Build a decision tree model for classification
decision_tree_model <- train(Y ~ ., data = train_data, method = "rpart")
# Build a decision tree model for classification
# Build a decision tree model for classification
# Check for missing values in the 'rank' column
missing_rank <- sum(is.na(train_data$rank))
# Print the number of missing values
cat("Number of missing values in 'rank':", missing_rank, "\n")
decision_tree_model <- train(Y ~ ., data = train_data, method = "rpart")
# Check for missing values in the 'rank' column
missing_rank <- sum(is.na(train_data$rank))
# Build a decision tree model for classification
# Build a decision tree model for classification
# Print the number of missing values
cat("Number of missing values in 'rank':", missing_rank, "\n")
decision_tree_model <- train(Y ~ ., data = train_data, method = "rpart")
# Impute missing values in the dataset
train_data <- na.omit(train_data)  # Removes rows with missing values
# Now, build the decision tree model
decision_tree_model <- train(Y ~ ., data = train_data, method = "rpart")
# Check for missing values in the 'rank' column
missing_rank <- sum(is.na(train_data$rank))
# Print the number of missing values
cat("Number of missing values in 'rank':", missing_rank, "\n")
# Remove rows with missing values
train_data <- na.omit(train_data)
# Now, build the decision tree model
decision_tree_model <- train(Y ~ ., data = train_data, method = "rpart")
# Create a new dataset without the 'rank' column
train_data_no_rank <- subset(train_data, select = -rank)
# Build the decision tree model with the modified dataset
decision_tree_model <- train(Y ~ ., data = train_data_no_rank, method = "rpart")
# Remove the 'rank' and 'Youtuber' columns
train_data <- train_data[, !(colnames(train_data) %in% c('rank', 'Youtuber'))]
test_data <- test_data[, !(colnames(test_data) %in% c('rank', 'Youtuber'))]
# Build the decision tree model
decision_tree_model <- train(high_subscribers ~ ., data = train_data, method = "rpart")
# Create a new column to classify subscribers as "high" or "low"
data$Subscriber_Class <- ifelse(data$subscribers > 1000000, "high", "low")
# Create the decision tree model
tree_model <- rpart(Subscriber_Class ~ video_views + category + uploads + Country,
data = data,
method = "class")
# Create a new column to classify subscribers as "high" or "low"
data$Subscriber_Class <- ifelse(data$subscribers > 1000000, "high", "low")
# Create the decision tree model
tree_model <- rpart(Subscriber_Class ~ video.views + category + uploads + Country,
data = data,
method = "class")
# Create a new column to classify subscribers as "high" or "low"
data$Subscriber_Class <- ifelse(data$subscribers > 1000000, "high", "low")
# Create the decision tree model
tree_model <- rpart(Subscriber_Class ~ video.views ,data = data,method = "class")
# Create a new column to classify subscribers as "high" or "low"
data$Subscriber_Class <- ifelse(data$subscribers > 1000000, "high", "low")
# Create the decision tree model
tree_model <- rpart(Subscriber_Class ~ video.views ,method = "class")
# Create a new column to classify subscribers as "high" or "low"
data$Subscriber_Class <- ifelse(data$subscribers > 1000000, "high", "low")
# Create the decision tree model
tree_model <- rpart(data$Subscriber_Class ~ video.views ,method = "class")
# Create a new column to classify subscribers as "high" or "low"
data$Subscriber_Class <- ifelse(data$subscribers > 1000000, "high", "low")
# Create the decision tree model
tree_model <- rpart(data$Subscriber_Class ~ data$video.views ,method = "class")
# Create a new column to classify subscribers as "high" or "low"
data$Subscriber_Class <- ifelse(data$subscribers > 1000000, "high", "low")
# Load necessary libraries
library(caret)
# Create a binary target variable for classification based on 'subscribers'
data$high_subscribers <- ifelse(data$subscribers > 100000, "High", "Low")
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$high_subscribers, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
data$Y <- as.factor(data$Y)
# Decision tree with party
library(party)
mytree <- ctree(Y~ video.views + category + uploads + Country,, data, controls=ctree_control(mincriterion=0.9, minsplit=50))
data$Y <- as.factor(data$Y)
# Decision tree with party
library(party)
mytree <- ctree(Y~ video.views + category + uploads + Country,data, controls=ctree_control(mincriterion=0.9, minsplit=50))
data$Y <- as.factor(data$Y)
# Decision tree with party
library(party)
mytree <- ctree(Y~ video.views + uploads ,data, controls=ctree_control(mincriterion=0.9, minsplit=50))
plot(mytree,type="simple")
# Misclassification error
tab <- table(predict(mytree), data$NSPF)
data$Y <- as.factor(data$Y)
# Decision tree with party
library(party)
mytree <- ctree(Y~ video.views + uploads ,data, controls=ctree_control(mincriterion=0.9, minsplit=50))
plot(mytree,type="simple")
# Misclassification error
tab <- table(predict(mytree), data$Y)
1-sum(diag(tab))/sum(tab)
data$Y <- as.factor(data$Y)
# Decision tree with party
library(party)
mytree <- ctree(Y~ video.views + uploads ,data, controls=ctree_control(mincriterion=0.9, minsplit=50))
plot(mytree,type="simple")
# Misclassification error
tab <- table(predict(mytree), data$Y)
tab
data$Y <- as.factor(data$Y)
# Decision tree with party
library(party)
mytree <- ctree(Y~ video.views + uploads ,data, controls=ctree_control(mincriterion=0.9, minsplit=50))
plot(mytree,type="simple")
# Misclassification error
tab <- table(predict(mytree), data$Y)
1-sum(diag(tab))/sum(tab)
data$Y <- as.factor(data$Y)
# Decision tree with party
library(party)
mytree <- ctree(Y~ video.views + uploads +lowest_monthly_earnings+video_views_for_the_last_30_days+video_views_rank+country_rank ,data, controls=ctree_control(mincriterion=0.9, minsplit=50))
plot(mytree,type="simple")
# Misclassification error
tab <- table(predict(mytree), data$Y)
1-sum(diag(tab))/sum(tab)
# Load necessary libraries
library(caret)
library(glm2)
target = Y
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Load necessary libraries
library(caret)
library(glm2)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
# Load necessary libraries
library(caret)
library(glm2)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Check for missing values in the 'target' and 'subscribers' columns
any(is.na(data$target))
any(is.na(data$subscribers))
# If there are missing values, you may need to handle them.
# For example, you can remove rows with missing values:
data <- na.omit(data)
# Now, split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
# Load necessary libraries
library(stats)
# Standardize or normalize your feature variables
standardized_data <- scale(data$subscribers)
# Determine the number of clusters (k)
# Use the Elbow Method, Silhouette Score, or other techniques to find the optimal k
# Apply k-means clustering
k <- 3  # Replace with your chosen k value
cluster_model <- kmeans(standardized_data, centers = k)
# Load necessary libraries
data=read.csv('data.csv')
Y=data$subscribers
library(caret)
library(glm2)
# Prepare your data
data$target <- ifelse(data$Y > 1000000, 1, 0)  # Define a binary target variable based on some threshold
# Check for missing values in the 'target' and 'subscribers' columns
any(is.na(data$target))
any(is.na(data$subscribers))
# If there are missing values, you may need to handle them.
# For example, you can remove rows with missing values:
data <- na.omit(data)
# Now, split the data into training and test sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure consistent levels for the target variable in both datasets
train_data$target <- factor(train_data$target, levels = c(0, 1))
test_data$target <- factor(test_data$target, levels = c(0, 1))
# Train a logistic regression model
logistic_model <- train(target ~ subscribers, data = train_data, method = "glm", family = "binomial")
print(logistic_model)
# Interpret and explain the logistic regression model
# Analyze the impact of "subscribers" on the binary target
# Load necessary libraries
library(stats)
data=read.csv('data.csv')
# Standardize or normalize your feature variables
standardized_data <- scale(data$subscribers)
# Determine the number of clusters (k)
# Use the Elbow Method, Silhouette Score, or other techniques to find the optimal k
# Apply k-means clustering
k <- 3  # Replace with your chosen k value
cluster_model <- kmeans(standardized_data, centers = k)
# Visualize the clustering results
plot(standardized_data, col = cluster_model$cluster)
# Interpret and explain the clusters
# Analyze cluster characteristics and patterns
